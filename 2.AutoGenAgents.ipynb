{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "- Assistant Agent  \n",
    "TOOL : autogen ì •ë³´ ì›¹ì„œì¹˜ , ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ (íšŒì‚¬ì •ë³´)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import StructuredMessage, TextMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken \n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "#tool ì •ì˜\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\"\n",
    "\n",
    "async def database_search(query: str) -> str:\n",
    "    \"\"\"Search a database for information\"\"\"\n",
    "    return \"The database contains information about the company's employees.\"\n",
    "\n",
    "#ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key= os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search, database_search], #multi-tool ì‚¬ìš©\n",
    "    system_message=\"\"\"ë‹¹ì‹ ì€ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ì‚¬ìš©ìë¥¼ ë•ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìì˜ ìš”ì²­ì„ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    1. ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤í† ì  ì˜ ì •ë³´ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "    2. ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ë‹µë³€: AutoGen is a programming framework for building multi-agent applications.\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run() -> None:\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=\"ì˜¤í† ì  ì˜ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ì–´\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    print(f\"ìµœì¢… ë‹µë³€: {response.chat_message.content}\")\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run()) when running in a script.\n",
    "await assistant_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ë‹µë³€: The database contains information about the company's employees.\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run2() -> None:\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=\"ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì˜¤í† ì  ì˜ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ì–´\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    print(f\"ìµœì¢… ë‹µë³€: {response.chat_message.content}\")\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run()) when running in a script.\n",
    "await assistant_run2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(chat_message=ToolCallSummaryMessage(source='weather_assistant', models_usage=None, metadata={}, content='ì„œìš¸ì˜ í˜„ì¬ ê¸°ì˜¨ì€ 22Â°C, ë§‘ìŒ', type='ToolCallSummaryMessage'), inner_messages=[ToolCallRequestEvent(source='weather_assistant', models_usage=RequestUsage(prompt_tokens=104, completion_tokens=5), metadata={}, content=[FunctionCall(id='', arguments='{\"location\":\"ì„œìš¸\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='weather_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='ì„œìš¸ì˜ í˜„ì¬ ê¸°ì˜¨ì€ 22Â°C, ë§‘ìŒ', name='get_weather', call_id='', is_error=False)], type='ToolCallExecutionEvent')])\n",
      "ì„œìš¸ì˜ í˜„ì¬ ê¸°ì˜¨ì€ 22Â°C, ë§‘ìŒ\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "#ê³ ì •ëœ ê°’ì„ ë°›ì•„ì•¼í•˜ëŠ” ê²½ìš°ë¼ë©´?! \n",
    "\n",
    "# ë‚ ì”¨ ì •ë³´ ë„êµ¬ í•¨ìˆ˜ ì •ì˜\n",
    "async def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        location: ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•  ë„ì‹œ ë˜ëŠ” ì§€ì—­ ì´ë¦„\n",
    "        unit: ì˜¨ë„ ë‹¨ìœ„ (celsius ë˜ëŠ” fahrenheit)\n",
    "    \n",
    "    Returns:\n",
    "        í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # ì‹¤ì œë¡œëŠ” ë‚ ì”¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ í•˜ì§€ë§Œ, ì˜ˆì œì—ì„œëŠ” ë”ë¯¸ ë°ì´í„° ë°˜í™˜\n",
    "    if location.lower() == \"ì„œìš¸\":\n",
    "        return \"ì„œìš¸ì˜ í˜„ì¬ ê¸°ì˜¨ì€ 22Â°C, ë§‘ìŒ\"\n",
    "    elif location.lower() == \"ë‰´ìš•\":\n",
    "        return \"ë‰´ìš•ì˜ í˜„ì¬ ê¸°ì˜¨ì€ 18Â°C, íë¦¼\"\n",
    "    else:\n",
    "        return f\"{location}ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "model_client = OpenAIChatCompletionClient(model='gemini-2.0-flash',api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ë‚ ì”¨ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ ìƒì„±\n",
    "weather_agent = AssistantAgent(\n",
    "    name=\"weather_assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"ì‚¬ìš©ìì˜ ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ì— ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ì‚¬ìš© ì˜ˆì‹œ\n",
    "async def run_weather_agent():\n",
    "    response = await weather_agent.on_messages(\n",
    "        [TextMessage(content=\"ì„œìš¸ì˜ ë‚ ì”¨ê°€ ì–´ë•Œ?\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken()\n",
    "    )\n",
    "    print(response)\n",
    "    print(response.chat_message.content)\n",
    "\n",
    "await run_weather_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ ì „ ì„¸ê³„ ì£¼ìš” ë„ì‹œ ë‚ ì”¨ ë¦¬í¬íŠ¸ ğŸŒ\n",
      "==================================================\n",
      "\n",
      "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ğŸŒ ë§‘ê³  ê¸°ì˜¨ì€ 22Â°Cì…ë‹ˆë‹¤. ë‚¨ì‚°íƒ€ì›Œê°€ ìš°ëš ì†Ÿì•„ìˆëŠ” ëœë“œë§ˆí¬ì´ë©° ğŸ™ï¸, K-popì˜ ì¤‘ì‹¬ì§€ë‹µê²Œ ğŸ¤ í™œê¸°ì°¬ ë„ì‹œ ë¶„ìœ„ê¸°ê°€ ëŠê»´ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ë‰´ìš•ì˜ ë‚ ì”¨ëŠ” â˜ï¸ íë¦¬ê³  ê¸°ì˜¨ì€ 18Â°Cì…ë‹ˆë‹¤. ììœ ì˜ ì—¬ì‹ ìƒì´ ğŸ—½ ì›…ì¥í•˜ê²Œ ì„œ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë¬¸í™”ê°€ ê³µì¡´í•˜ëŠ”  melting pot ğŸ‡ºğŸ‡³ ê°™ì€ ë„ì‹œì…ë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ë„ì¿„ì˜ ë‚ ì”¨ëŠ” ğŸŒ§ï¸ ë¹„ê°€ ë‚´ë¦¬ê³  ê¸°ì˜¨ì€ 20Â°Cì…ë‹ˆë‹¤. ë„ì¿„íƒ€ì›Œê°€ ğŸ—¼ ì€ì€í•˜ê²Œ ë¹›ë‚˜ê³  ìˆìœ¼ë©°, ì „í†µì ì¸ ì‚¬ì°°ê³¼ í˜„ëŒ€ì ì¸ ê³ ì¸µ ë¹Œë”©ì´ ğŸ¯ğŸ™ï¸ ì¡°í™”ë¡­ê²Œ ì–´ìš°ëŸ¬ì§„ ë„ì‹œì…ë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "íŒŒë¦¬ì˜ ë‚ ì”¨ëŠ” ğŸŒ ë§‘ê³  ê¸°ì˜¨ì€ 15Â°Cì…ë‹ˆë‹¤. ì—í íƒ‘ì´ ğŸ—¼ ì•„ë¦„ë‹¤ìš´ ìíƒœë¥¼ ë½ë‚´ê³  ìˆìœ¼ë©°, ì˜ˆìˆ  ì‘í’ˆê³¼ ğŸ¨ ì„¸ë ¨ëœ íŒ¨ì…˜ ê°ê°ì´ ğŸ’ƒ ë„˜ì¹˜ëŠ” ë„ì‹œì…ë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import CancellationToken\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# ë‚ ì”¨ ì •ë³´ ë„êµ¬ í•¨ìˆ˜ ì •ì˜\n",
    "async def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    weather_data = {\n",
    "        \"ì„œìš¸\": {\"temp\": 22, \"condition\": \"ë§‘ìŒ\", \"landmark\": \"ë‚¨ì‚°íƒ€ì›Œ\", \"culture\": \"K-popì˜ ì¤‘ì‹¬ì§€\"},\n",
    "        \"ë‰´ìš•\": {\"temp\": 18, \"condition\": \"íë¦¼\", \"landmark\": \"ììœ ì˜ ì—¬ì‹ ìƒ\", \"culture\": \"ë¬¸í™” ë‹¤ì–‘ì„±ì˜ ë„ì‹œ\"},\n",
    "        \"ë„ì¿„\": {\"temp\": 20, \"condition\": \"ë¹„\", \"landmark\": \"ë„ì¿„íƒ€ì›Œ\", \"culture\": \"ì „í†µê³¼ í˜„ëŒ€ì˜ ì¡°í™”\"},\n",
    "        \"íŒŒë¦¬\": {\"temp\": 15, \"condition\": \"ë§‘ìŒ\", \"landmark\": \"ì—í íƒ‘\", \"culture\": \"ì˜ˆìˆ ê³¼ íŒ¨ì…˜ì˜ ë„ì‹œ\"}\n",
    "    }\n",
    "    \n",
    "    location = location.lower()\n",
    "    if location in weather_data:\n",
    "        return weather_data[location]\n",
    "    return None\n",
    "\n",
    "class WeatherEmojiAgent:\n",
    "    def __init__(self):\n",
    "        self.weather_agent = AssistantAgent(\n",
    "            name=\"weather_assistant\",\n",
    "            model_client=model_client,\n",
    "            tools=[get_weather],\n",
    "            system_message=\"ì‚¬ìš©ìì˜ ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ì— ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "        \n",
    "        self.emoji_agent = AssistantAgent(\n",
    "            name=\"emoji_assistant\",\n",
    "            model_client=model_client,\n",
    "            system_message=\"\"\"ë‚ ì”¨ ì •ë³´ë¥¼ ë°›ì•„ì„œ ì ì ˆí•œ ì´ëª¨ì§€ì™€ í•¨ê»˜ ë” í’ë¶€í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "            ë‚ ì”¨ ì¡°ê±´ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n",
    "            - ë§‘ìŒ: ğŸŒ\n",
    "            - íë¦¼: â˜ï¸\n",
    "            - ë¹„: ğŸŒ§ï¸\n",
    "            ë„ì‹œì˜ íŠ¹ì§•ë„ ì´ëª¨ì§€ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "        )\n",
    "    \n",
    "    async def get_weather_info(self, location: str) -> str:\n",
    "        # ë‚ ì”¨ ì •ë³´ ì¡°íšŒ\n",
    "        weather_response = await self.weather_agent.on_messages(\n",
    "            [TextMessage(content=f\"{location}ì˜ ë‚ ì”¨ê°€ ì–´ë•Œ?\", source=\"user\")],\n",
    "            cancellation_token=CancellationToken()\n",
    "        )\n",
    "        \n",
    "        weather_data = await get_weather(location)\n",
    "        if not weather_data:\n",
    "            return f\"âŒ {location}ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "            \n",
    "        # ì´ëª¨ì§€ ì—ì´ì „íŠ¸ì—ê²Œ ë‚ ì”¨ ì •ë³´ ì „ë‹¬\n",
    "        emoji_prompt = f\"\"\"\n",
    "        ë„ì‹œ: {location}\n",
    "        ê¸°ì˜¨: {weather_data['temp']}Â°C\n",
    "        ë‚ ì”¨: {weather_data['condition']}\n",
    "        ëœë“œë§ˆí¬: {weather_data['landmark']}\n",
    "        íŠ¹ì§•: {weather_data['culture']}\n",
    "        \"\"\"\n",
    "        \n",
    "        emoji_response = await self.emoji_agent.on_messages(\n",
    "            [TextMessage(content=emoji_prompt, source=\"user\")],\n",
    "            cancellation_token=CancellationToken()\n",
    "        )\n",
    "        \n",
    "        return emoji_response.chat_message.content\n",
    "\n",
    "async def main():\n",
    "    weather_emoji_agent = WeatherEmojiAgent()\n",
    "    cities = [\"ì„œìš¸\", \"ë‰´ìš•\", \"ë„ì¿„\", \"íŒŒë¦¬\"]\n",
    "    \n",
    "    print(\"\\nğŸŒ ì „ ì„¸ê³„ ì£¼ìš” ë„ì‹œ ë‚ ì”¨ ë¦¬í¬íŠ¸ ğŸŒ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for city in cities:\n",
    "        weather_info = await weather_emoji_agent.get_weather_info(city)\n",
    "        print(f\"\\n{weather_info}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•„í„°ë§ì„ í™œìš©í•œ ì—ì´ì „íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ›ï¸ ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ ì–´ì‹œìŠ¤í„´íŠ¸ ğŸ›ï¸\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¤ ì‚¬ìš©ì:  ì‡¼í•‘ëª°ì— ìˆëŠ” ì• í”Œ ë˜ëŠ” ì‚¼ì„±ì¤‘ì— ìµœì‹ ëª¨ë¸ì„ ì°¾ê³  ìˆì–´ ë‘ê°œì˜ íŠ¹ì¥ì ì„ ë¹„êµí•´ì¤˜\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸:\n",
      "ì•Œê² ìŠµë‹ˆë‹¤. ì• í”Œê³¼ ì‚¼ì„± ì œí’ˆ ì¤‘ ì–´ë–¤ ì¢…ë¥˜ì˜ ì œí’ˆì„ ì°¾ìœ¼ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´, í•´ë‹¹ ì œí’ˆêµ°ì˜ ìµœì‹  ëª¨ë¸ì„ ì°¾ì•„ íŠ¹ì¥ì ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì¶”ì²œí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤ë§ˆíŠ¸í°ì„ ì°¾ìœ¼ì‹ ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**í˜„ì¬ ì¬ê³ :** ì¢…ë¥˜ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ í™•ì¸ í›„ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ğŸ’¡ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:**\n",
      "    *   ì• í”Œê³¼ ì‚¼ì„±ì˜ ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "    *   ê° ëª¨ë¸ì˜ ì£¼ìš” ìŠ¤í™, ë””ìì¸, ì¹´ë©”ë¼ ì„±ëŠ¥, ë°°í„°ë¦¬ ìˆ˜ëª… ë“±ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "2.  **ğŸ† ìµœê³  ì¶”ì²œ ì œí’ˆ:**\n",
      "    *   ì¢…í•©ì ì¸ ì„±ëŠ¥, ë””ìì¸, ì‚¬ìš©ì ê²½í—˜ ë“±ì„ ê³ ë ¤í•˜ì—¬ ìµœê³  ì¶”ì²œ ì œí’ˆì„ ì„ ì •í•©ë‹ˆë‹¤.\n",
      "    *   ì„ ì • ì´ìœ ì™€ í•¨ê»˜ ì œí’ˆì˜ ì¥ì ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "3.  **ğŸ’° ê°€ì„±ë¹„ ì¶”ì²œ ì œí’ˆ:**\n",
      "    *   ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ì œí’ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
      "    *   ì£¼ìš” ìŠ¤í™ê³¼ ì¥ì ì„ ì„¤ëª…í•˜ê³ , ë¹„ìŠ·í•œ ê°€ê²©ëŒ€ì˜ ë‹¤ë¥¸ ì œí’ˆê³¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "4.  **âš ï¸ êµ¬ë§¤ ì‹œ ê³ ë ¤ì‚¬í•­:**\n",
      "    *   ì• í”Œê³¼ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í° ì„ íƒ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "    *   ìš´ì˜ì²´ì œ(iOS vs Android), ìƒíƒœê³„, ê°œì¸ì ì¸ ì„ í˜¸ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì–´ë–¤ ì¢…ë¥˜ì˜ ì œí’ˆì„ ì°¾ìœ¼ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´, ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì œí’ˆì„ ì¶”ì²œí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ˜Š\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import CancellationToken\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# ì‡¼í•‘ëª° ë°ì´í„°ë² ì´ìŠ¤\n",
    "product_database = {\n",
    "    \"ë…¸íŠ¸ë¶\": [\n",
    "        {\"id\": 1, \"name\": \"LG ê·¸ë¨ 16\", \"price\": 1500000, \"category\": \"ë…¸íŠ¸ë¶\", \"rating\": 4.7, \"in_stock\": True, \"stock_count\": 5},\n",
    "        {\"id\": 2, \"name\": \"ë§¥ë¶ í”„ë¡œ 14\", \"price\": 2500000, \"category\": \"ë…¸íŠ¸ë¶\", \"rating\": 4.9, \"in_stock\": True, \"stock_count\": 3},\n",
    "        {\"id\": 3, \"name\": \"ì‚¼ì„± ê°¤ëŸ­ì‹œë¶ 4\", \"price\": 1200000, \"category\": \"ë…¸íŠ¸ë¶\", \"rating\": 4.5, \"in_stock\": True, \"stock_count\": 10},\n",
    "        {\"id\": 4, \"name\": \"MSI ê²Œì´ë° ë…¸íŠ¸ë¶\", \"price\": 1800000, \"category\": \"ê²Œì´ë° ë…¸íŠ¸ë¶\", \"rating\": 4.6, \"in_stock\": False, \"stock_count\": 0},\n",
    "        {\"id\": 5, \"name\": \"ASUS ROG ê²Œì´ë°ë¶\", \"price\": 2200000, \"category\": \"ê²Œì´ë° ë…¸íŠ¸ë¶\", \"rating\": 4.8, \"in_stock\": True, \"stock_count\": 2}\n",
    "    ],\n",
    "    \"ìŠ¤ë§ˆíŠ¸í°\": [\n",
    "        {\"id\": 6, \"name\": \"ì•„ì´í° 15 Pro\", \"price\": 1350000, \"category\": \"ìŠ¤ë§ˆíŠ¸í°\", \"rating\": 4.8, \"in_stock\": True, \"stock_count\": 4},\n",
    "        {\"id\": 7, \"name\": \"ê°¤ëŸ­ì‹œ S23 Ultra\", \"price\": 1450000, \"category\": \"ìŠ¤ë§ˆíŠ¸í°\", \"rating\": 4.7, \"in_stock\": True, \"stock_count\": 6},\n",
    "        {\"id\": 8, \"name\": \"í”½ì…€ 8 Pro\", \"price\": 1200000, \"category\": \"ìŠ¤ë§ˆíŠ¸í°\", \"rating\": 4.6, \"in_stock\": True, \"stock_count\": 8}\n",
    "    ],\n",
    "    \"í—¤ë“œí°\": [\n",
    "        {\"id\": 9, \"name\": \"ì—ì–´íŒŸ í”„ë¡œ 2\", \"price\": 350000, \"category\": \"ì´ì–´í°\", \"rating\": 4.7, \"in_stock\": True, \"stock_count\": 15},\n",
    "        {\"id\": 10, \"name\": \"ì†Œë‹ˆ WH-1000XM5\", \"price\": 450000, \"category\": \"í—¤ë“œí°\", \"rating\": 4.9, \"in_stock\": True, \"stock_count\": 7}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ì œí’ˆ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "async def search_products(keyword: str, min_price: float = 0, max_price: float = 10000000) -> str:\n",
    "\n",
    "    results = []\n",
    "    found_keyword = False\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰\n",
    "    for key, products in product_database.items():\n",
    "        if keyword.lower() in key.lower():\n",
    "            found_keyword = True\n",
    "            for product in products:\n",
    "                if min_price <= product[\"price\"] <= max_price:\n",
    "                    results.append(product)\n",
    "    \n",
    "    # ì œí’ˆëª… ê²€ìƒ‰\n",
    "    if not found_keyword:\n",
    "        for products in product_database.values():\n",
    "            for product in products:\n",
    "                if (keyword.lower() in product[\"name\"].lower() and \n",
    "                    min_price <= product[\"price\"] <= max_price):\n",
    "                    results.append(product)\n",
    "    \n",
    "    # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…\n",
    "    if not results:\n",
    "        return f\"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.\"\n",
    "    \n",
    "    result_str = f\"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ ì œí’ˆ ë°œê²¬):\\n\\n\"\n",
    "    for product in results:\n",
    "        stock_status = f\"ì¬ê³  {product['stock_count']}ê°œ, \" + (\"ì¬ê³  ìˆìŒ âœ…\" if product[\"in_stock\"] else \"í’ˆì ˆ âŒ\")\n",
    "        result_str += f\"- {product['name']}\\n\"\n",
    "        result_str += f\"  ğŸ’° ê°€ê²©: {product['price']:,}ì›\\n\"\n",
    "        result_str += f\"  â­ í‰ì : {product['rating']}\\n\"\n",
    "        result_str += f\"  ğŸ“¦ {stock_status}\\n\\n\"\n",
    "    \n",
    "    return result_str\n",
    "\n",
    "class SmartShoppingAgent:\n",
    "    def __init__(self):\n",
    "        self.model_client = OpenAIChatCompletionClient(\n",
    "            model='gemini-2.0-flash',\n",
    "            api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # ì œí’ˆ ê²€ìƒ‰ ì—ì´ì „íŠ¸\n",
    "        self.search_agent = AssistantAgent(\n",
    "            name=\"search_assistant\",\n",
    "            model_client=self.model_client,\n",
    "            tools=[search_products],\n",
    "            system_message=\"\"\"ì œí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "            ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê²€ìƒ‰í•˜ê³ ,\n",
    "            ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "            ê°€ê²©ëŒ€ê°€ ì§€ì •ëœ ê²½ìš° ë°˜ë“œì‹œ í•´ë‹¹ ë²”ìœ„ ë‚´ì˜ ì œí’ˆë§Œ ê²€ìƒ‰í•˜ì„¸ìš”.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # ë¶„ì„ ë° ì¶”ì²œ ì—ì´ì „íŠ¸\n",
    "        self.recommendation_agent = AssistantAgent(\n",
    "            name=\"recommendation_assistant\",\n",
    "            model_client=self.model_client,\n",
    "            system_message=\"\"\"ì‡¼í•‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "            ê²€ìƒ‰ëœ ì œí’ˆë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”:\n",
    "            \n",
    "            1. ğŸ’ ì¢…í•© í‰ê°€\n",
    "               - ê°€ê²©, í‰ì , ì¬ê³  ìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤\n",
    "               \n",
    "            2. ğŸ’° ê°€ì„±ë¹„ ë¶„ì„\n",
    "               - ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ê³¼ í‰ì  ë¶„ì„\n",
    "               \n",
    "            3. ğŸ¯ êµ¬ë§¤ ê°€ì´ë“œ\n",
    "               - ì œí’ˆ ì„ íƒ ì‹œ ê³ ë ¤ì‚¬í•­\n",
    "               - ë¹„êµ ì¥ë‹¨ì \n",
    "               \n",
    "            í•­ìƒ ê°ê´€ì ì¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•˜ê³ ,\n",
    "            ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì´ëª¨ì§€ì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "        )\n",
    "\n",
    "    async def process_query(self, user_query: str) -> str:\n",
    "        # 1ë‹¨ê³„: ì œí’ˆ ê²€ìƒ‰\n",
    "        search_response = await self.search_agent.on_messages(\n",
    "            [TextMessage(content=user_query, source=\"user\")],\n",
    "            cancellation_token=CancellationToken()\n",
    "        )\n",
    "        \n",
    "        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°\n",
    "        if \"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\" in search_response.chat_message.content:\n",
    "            return f\"\"\"\n",
    "            ğŸ˜¢ ì£„ì†¡í•©ë‹ˆë‹¤!\n",
    "            \n",
    "            {search_response.chat_message.content}\n",
    "            \n",
    "            ğŸ’¡ ì¶”ì²œ ê²€ìƒ‰ì–´:\n",
    "            - ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰: ë…¸íŠ¸ë¶, ìŠ¤ë§ˆíŠ¸í°, í—¤ë“œí°\n",
    "            - ê°€ê²©ëŒ€ë¥¼ ì§€ì •í•˜ì—¬ ê²€ìƒ‰\n",
    "            - ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰\n",
    "            \"\"\"\n",
    "        \n",
    "        # 2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ë° ì¶”ì²œ\n",
    "        recommendation_prompt = f\"\"\"\n",
    "        ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì œí’ˆì„ ì¶”ì²œí•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n",
    "        \n",
    "        ê²€ìƒ‰ ê²°ê³¼:\n",
    "        {search_response.chat_message.content}\n",
    "        \n",
    "        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "        í˜„ì¬ ì¬ê³  : \n",
    "        1. ğŸ’¡ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½\n",
    "        2. ğŸ† ìµœê³  ì¶”ì²œ ì œí’ˆ\n",
    "        3. ğŸ’° ê°€ì„±ë¹„ ì¶”ì²œ ì œí’ˆ\n",
    "        4. âš ï¸ êµ¬ë§¤ì‹œ ê³ ë ¤ì‚¬í•­\n",
    "        \"\"\"\n",
    "        \n",
    "        final_response = await self.recommendation_agent.on_messages(\n",
    "            [TextMessage(content=recommendation_prompt, source=\"user\")],\n",
    "            cancellation_token=CancellationToken()\n",
    "        )\n",
    "        \n",
    "        return final_response.chat_message.content\n",
    "\n",
    "async def main():\n",
    "    shopping_agent = SmartShoppingAgent()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì˜ˆì‹œ\n",
    "    test_queries = [\n",
    "        \"\"\" ì‡¼í•‘ëª°ì— ìˆëŠ” ì• í”Œ ë˜ëŠ” ì‚¼ì„±ì¤‘ì— ìµœì‹ ëª¨ë¸ì„ ì°¾ê³  ìˆì–´ ë‘ê°œì˜ íŠ¹ì¥ì ì„ ë¹„êµí•´ì¤˜\"\"\"\n",
    "\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ›ï¸ ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ ì–´ì‹œìŠ¤í„´íŠ¸ ğŸ›ï¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nğŸ‘¤ ì‚¬ìš©ì: {query}\")\n",
    "        print(\"-\" * 60)\n",
    "        response = await shopping_agent.process_query(query)\n",
    "        print(f\"\\nğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸:\\n{response}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒ€ ìƒì„±\n",
    "\n",
    "## RoundRobinGroupChat\n",
    "- ì°¸ê°€ìë“¤ì´ ìˆœì°¨ì ìœ¼ë¡œ ë²ˆê°ˆì•„ê°€ë©° ë©”ì‹œì§€ë¥¼ ì£¼ê³ ë°›ëŠ” íŒ€ì…ë‹ˆë‹¤. íŠ¹ì • ë©”ì‹œì§€ë‚˜ ì•¡ì…˜ì´ ë°œìƒí•  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "1. `TextMentionTermination(\"String\")` - ë©”ì‹œì§€ ë“±ì¥ ê¸°ë°˜ ì¢…ë£Œ ì¡°ê±´\n",
    "2. `MaxMessageTermination(int)` - í˜¸ì¶œ íšŸìˆ˜ ê¸°ë°˜ ì¢…ë£Œ ì¡°ê±´\n",
    "3. `ExternalTermination` - `cancellation_token.cancel()`ì„ í™œìš©í•œ ì¢…ë£Œ ì¡°ê±´\n",
    "4. `CompositeTermination` - `OrTermination`ì˜ ë³µí•© ì¡°ê±´\n",
    "\n",
    "ğŸ› ï¸ **ì¶”ê°€ íŒ**: `reset()`ê³¼ `pause()` / `resume()` ë©”ì„œë“œ í™œìš©\n",
    "- `await team.pause()` : ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì¤‘ì§€í•©ë‹ˆë‹¤ (ì¬ê°œ ê°€ëŠ¥)\n",
    "- `await team.resume()` : ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ë‹¤ì‹œ ì¬ê°œí•©ë‹ˆë‹¤\n",
    "- `await team.reset()` : ìƒíƒœë¥¼ ì´ˆê¸°í™”í•œ í›„ ì¬ì‹œì‘í•©ë‹ˆë‹¤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "ê³ ì–‘ì´ê°€ ì£¼ì¸ê³µì¸ ì§§ì€ ëª¨í—˜ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_story_team\u001b[39m():\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[32m     63\u001b[39m         team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mê³ ì–‘ì´ê°€ ì£¼ì¸ê³µì¸ ì§§ì€ ëª¨í—˜ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     64\u001b[39m         output_stats=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_story_team()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mrun_story_team\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_story_team\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[32m     63\u001b[39m         team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mê³ ì–‘ì´ê°€ ì£¼ì¸ê³µì¸ ì§§ì€ ëª¨í—˜ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     64\u001b[39m         output_stats=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:498\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    496\u001b[39m     cancellation_token.link_future(message_future)\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Wait for the next message, this will raise an exception if the task is cancelled.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m message = \u001b[38;5;28;01mawait\u001b[39;00m message_future\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/asyncio/queues.py:158\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._getters.append(getter)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    160\u001b[39m     getter.cancel()  \u001b[38;5;66;03m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.base import TaskResult\n",
    "from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination, MaxMessageTermination\n",
    "\n",
    "\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "# ê¸°ë³¸ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"\n",
    ")\n",
    "\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”. í”¼ë“œë°±ì´ ë°˜ì˜ë˜ë©´ 'APPROVE'ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "#ì¢…ë£Œ ì¡°ê±´ ì •ì˜ - 'APPROVE' ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì¢…ë£Œ\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# ì˜ˆì‹œ: ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ê°€ ë„ë‹¬í•˜ë©´ ì¢…ë£Œ\n",
    "# from autogen_agentchat.conditions import MaxMessageTermination\n",
    "# text_termination = MaxMessageTermination(10)\n",
    "\n",
    "# ì˜ˆì‹œ: ì™¸ë¶€ ì´ë²¤íŠ¸ë¡œ ì¢…ë£Œ ì œì–´\n",
    "# from autogen_core import CancellationToken\n",
    "# cancellation_token = CancellationToken()\n",
    "# run_task = asyncio.create_task(\n",
    "#     team.run_stream(task=\"...\", cancellation_token=cancellation_token)\n",
    "# )\n",
    "# await asyncio.sleep(5)\n",
    "# cancellation_token.cancel()\n",
    "\n",
    "# ì˜ˆì‹œ: ë³µí•© ì¡°ê±´ (10í„´ì´ê±°ë‚˜ \"DONE\" í¬í•¨ì‹œ ì¢…ë£Œ)\n",
    "# from autogen_agentchat.conditions import OrTermination\n",
    "# text_termination = OrTermination([\n",
    "#     MaxMessageTermination(10),\n",
    "#     TextMentionTermination(\"DONE\")\n",
    "# ])\n",
    "# text_termination = TextMentionTermination(\"APPROVE\")\n",
    "#\n",
    "\n",
    "# íŒ€ ìƒì„±\n",
    "team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)\n",
    "\n",
    "# íŒ€ ì‹¤í–‰\n",
    "# íŒ€ ì‹¤í–‰\n",
    "async def run_story_team():\n",
    "    await Console(\n",
    "        team.run_stream(task=\"ê³ ì–‘ì´ê°€ ì£¼ì¸ê³µì¸ ì§§ì€ ëª¨í—˜ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"),\n",
    "        output_stats=True\n",
    "    )\n",
    "await run_story_team()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ SelectorGroupChat - ì„ íƒ ê¸°ë°˜ íŒ€ ğŸŒŸ\n",
    "- ê° ë©”ì‹œì§€ í›„ ChatCompletion ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë°œì–¸ìë¥¼ ì„ íƒí•˜ëŠ” íŒ€ì…ë‹ˆë‹¤. ğŸ’¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë‹¤ìŒ ì—¬í–‰ ìƒë‹´ ëŒ€í™”ë¥¼ ì½ê³ , í˜„ì¬ ëŒ€í™” ë§¥ë½ì—ì„œ ê°€ì¥ ì í•©í•œ ë‹¤ìŒ ë°œì–¸ìë¥¼ ì„ íƒí•˜ì„¸ìš”:\n",
      "\n",
      "    ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€:\n",
      "    {participants}\n",
      "\n",
      "    ëŒ€í™” ë‚´ì—­:\n",
      "    {history}\n",
      "\n",
      "    ìœ„ ëŒ€í™”ì—ì„œ ë‹¤ìŒìœ¼ë¡œ ëˆ„ê°€ ë°œì–¸í•´ì•¼ ê°€ì¥ ì ì ˆí•œì§€ íŒë‹¨í•˜ì—¬, í•´ë‹¹ ì „ë¬¸ê°€ì˜ ì´ë¦„ë§Œ ì •í™•íˆ ë°˜í™˜í•˜ì„¸ìš”.\n",
      "    \n",
      "---------- user ----------\n",
      "ì„œìš¸ì—ì„œ ì œì£¼ë„ë¡œ 3ì¼ê°„ ì—¬í–‰ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”. í™œë™ì ì¸ íœ´ê°€ë¥¼ ì›í•˜ë©°, ì¤‘ê¸‰ ìˆ˜ì¤€ì˜ ìˆ™ì†Œë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "async def selector_team_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    # ê° ì „ë¬¸ê°€ ì—­í• ì„ í•˜ëŠ” ì—ì´ì „íŠ¸ë“¤ ìƒì„±\n",
    "    planner = AssistantAgent(\n",
    "        \"Planner\",\n",
    "        model_client,\n",
    "        description=\"ì „ì²´ ì—¬í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€\",\n",
    "        system_message=\"ë‹¹ì‹ ì€ ì—¬í–‰ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì „ì²´ì ì¸ ì—¬í–‰ ê³„íšê³¼ ì¼ì •ì„ ìˆ˜ë¦½í•˜ì„¸ìš”. ìˆ™ì†Œë‚˜ êµí†µì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° í•´ë‹¹ ì „ë¬¸ê°€ì—ê²Œ ì§ˆë¬¸í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    \n",
    "    accommodation = AssistantAgent(\n",
    "        \"Accommodation_Expert\",\n",
    "        model_client,\n",
    "        description=\"ìˆ™ì†Œ ë° í˜¸í…” ì˜ˆì•½ ì „ë¬¸ê°€\",\n",
    "        system_message=\"ë‹¹ì‹ ì€ ìˆ™ì†Œ ì˜ˆì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìœ„ì¹˜, ê°€ê²©ëŒ€, ì‹œì„¤ ë“± ìˆ™ì†Œì— ê´€í•œ ìƒì„¸í•œ ì •ë³´ì™€ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    \n",
    "    transport = AssistantAgent(\n",
    "        \"Transport_Expert\",\n",
    "        model_client,\n",
    "        description=\"êµí†µ ë° ì´ë™ ìˆ˜ë‹¨ ì „ë¬¸ê°€\",\n",
    "        system_message=\"ë‹¹ì‹ ì€ êµí†µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ê³µ, ê¸°ì°¨, ë²„ìŠ¤, ë Œí„°ì¹´ ë“± ì´ë™ ìˆ˜ë‹¨ì— ê´€í•œ ìƒì„¸ ì •ë³´ì™€ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    \n",
    "    activities = AssistantAgent(\n",
    "        \"Activities_Expert\",\n",
    "        model_client,\n",
    "        description=\"ê´€ê´‘ ëª…ì†Œ ë° ì•¡í‹°ë¹„í‹° ì „ë¬¸ê°€\",\n",
    "        system_message=\"ë‹¹ì‹ ì€ ê´€ê´‘ ë° ì•¡í‹°ë¹„í‹° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°©ë¬¸í•  ë§Œí•œ ëª…ì†Œ, ì²´í—˜, ì‹ë‹¹ ë“±ì„ ì¶”ì²œí•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    \n",
    "    # ë§ì¶¤í˜• ì„ íƒê¸° í”„ë¡¬í”„íŠ¸ ì‘ì„±\n",
    "    selector_prompt = \"\"\"\n",
    "    ë‹¤ìŒ ì—¬í–‰ ìƒë‹´ ëŒ€í™”ë¥¼ ì½ê³ , í˜„ì¬ ëŒ€í™” ë§¥ë½ì—ì„œ ê°€ì¥ ì í•©í•œ ë‹¤ìŒ ë°œì–¸ìë¥¼ ì„ íƒí•˜ì„¸ìš”:\n",
    "    \n",
    "    ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€:\n",
    "    {participants}\n",
    "    \n",
    "    ëŒ€í™” ë‚´ì—­:\n",
    "    {history}\n",
    "    \n",
    "    ìœ„ ëŒ€í™”ì—ì„œ ë‹¤ìŒìœ¼ë¡œ ëˆ„ê°€ ë°œì–¸í•´ì•¼ ê°€ì¥ ì ì ˆí•œì§€ íŒë‹¨í•˜ì—¬, í•´ë‹¹ ì „ë¬¸ê°€ì˜ ì´ë¦„ë§Œ ì •í™•íˆ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    " \n",
    "    # ì¢…ë£Œ ì¡°ê±´ ì„¤ì •\n",
    "    termination = TextMentionTermination(\"PLANNING_COMPLETE\")\n",
    "    \n",
    "    # SelectorGroupChat íŒ€ ìƒì„±\n",
    "    team = SelectorGroupChat(\n",
    "        [planner, accommodation, transport, activities],\n",
    "        model_client=model_client,\n",
    "        selector_prompt=selector_prompt,\n",
    "        termination_condition=termination,\n",
    "    )\n",
    "    \n",
    "    # íŒ€ ì‹¤í–‰\n",
    "    await Console(\n",
    "        team.run_stream(task=\"ì„œìš¸ì—ì„œ ì œì£¼ë„ë¡œ 3ì¼ê°„ ì—¬í–‰ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”. í™œë™ì ì¸ íœ´ê°€ë¥¼ ì›í•˜ë©°, ì¤‘ê¸‰ ìˆ˜ì¤€ì˜ ìˆ™ì†Œë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.\"),\n",
    "        output_stats=True\n",
    "    )\n",
    "\n",
    "await selector_team_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm - í•¸ë“œì˜¤í”„ ê¸°ë°˜ íŒ€\n",
    "í˜„ì¬ í™”ìê°€ HandOfMessage ë¥¼ ë³´ë‚´ ë‹¤ìŒ í™”ìë¥¼ ì§€ì • (ëª…ì‹œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "ë¬¼ì²´ê°€ 100m ë†’ì´ì—ì„œ ë–¨ì–´ì§ˆ ë•Œ ë°”ë‹¥ì— ë„ë‹¬í•˜ëŠ” ì‹œê°„ê³¼ ì¶©ëŒ ì†ë„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.\n",
      "---------- Math_Expert ----------\n",
      "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ ë¬¼ë¦¬í•™ì  ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. Physics_Expertì—ê²Œ ì´ ì§ˆë¬¸ì„ ë„˜ê¸°ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "---------- Math_Expert ----------\n",
      "[FunctionCall(id='', arguments='{}', name='transfer_to_physics_expert')]\n",
      "---------- Math_Expert ----------\n",
      "[FunctionExecutionResult(content='Transferred to Physics_Expert, adopting the role of Physics_Expert immediately.', name='transfer_to_physics_expert', call_id='', is_error=False)]\n",
      "---------- Math_Expert ----------\n",
      "Transferred to Physics_Expert, adopting the role of Physics_Expert immediately.\n",
      "---------- Physics_Expert ----------\n",
      "ììœ  ë‚™í•˜ ìš´ë™ì„ ê°€ì •í•˜ê³  ê³µê¸° ì €í•­ì„ ë¬´ì‹œí•˜ë©´ ë‹¤ìŒ ê³µì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì‹œê°„(t)ì„ ê³„ì‚°í•˜ë ¤ë©´:\n",
      "h = (1/2) * g * t^2\n",
      "ì—¬ê¸°ì„œ hëŠ” ë†’ì´(100m), gëŠ” ì¤‘ë ¥ ê°€ì†ë„(ì•½ 9.8m/s^2)ì…ë‹ˆë‹¤.\n",
      "ë”°ë¼ì„œ t = sqrt((2*h)/g)\n",
      "\n",
      "ì¶©ëŒ ì†ë„(v)ë¥¼ ê³„ì‚°í•˜ë ¤ë©´:\n",
      "v = g * t\n",
      "\n",
      "ì´ì œ ì‹¤ì œ ê³„ì‚°ì„ ìœ„í•´ Math_Expertì—ê²Œ ì§ˆë¬¸ì„ ë„˜ê¸°ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "---------- Physics_Expert ----------\n",
      "\n",
      "---------- Physics_Expert ----------\n",
      "[FunctionCall(id='', arguments='{}', name='transfer_to_math_expert')]\n",
      "---------- Physics_Expert ----------\n",
      "[FunctionExecutionResult(content='Transferred to Math_Expert, adopting the role of Math_Expert immediately.', name='transfer_to_math_expert', call_id='', is_error=False)]\n",
      "---------- Physics_Expert ----------\n",
      "Transferred to Math_Expert, adopting the role of Math_Expert immediately.\n",
      "---------- Math_Expert ----------\n",
      "ì‹œê°„(t)ì„ ê³„ì‚°í•˜ë ¤ë©´:\n",
      "t = sqrt((2*h)/g)\n",
      "h = 100mì´ê³  g = 9.8m/s^2ì´ë¯€ë¡œ\n",
      "t = sqrt((2*100)/9.8) = sqrt(200/9.8) = sqrt(20.408) â‰ˆ 4.518ì´ˆ\n",
      "\n",
      "ì¶©ëŒ ì†ë„(v)ë¥¼ ê³„ì‚°í•˜ë ¤ë©´:\n",
      "v = g * t\n",
      "g = 9.8m/s^2ì´ê³  t = 4.518ì´ˆì´ë¯€ë¡œ\n",
      "v = 9.8 * 4.518 â‰ˆ 44.276m/s\n",
      "\n",
      "ë”°ë¼ì„œ ë¬¼ì²´ëŠ” ì•½ 4.518ì´ˆ ë§Œì— ë°”ë‹¥ì— ë„ë‹¬í•˜ê³  ì¶©ëŒ ì†ë„ëŠ” ì•½ 44.276m/sì…ë‹ˆë‹¤.\n",
      "---------- Math_Expert ----------\n",
      "\n",
      "---------- Math_Expert ----------\n",
      "4.  518ì´ˆì™€ 44.276m/sëŠ” ìœ íš¨í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
      "ìµœì¢… ë‹µì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë¬¼ì²´ê°€ 100m ë†’ì´ì—ì„œ ë–¨ì–´ì§ˆ ë•Œ ì•½ 4.518ì´ˆ ë§Œì— ë°”ë‹¥ì— ë„ë‹¬í•˜ê³  ì¶©ëŒ ì†ë„ëŠ” ì•½ 44.276m/sì…ë‹ˆë‹¤.\n",
      "---------- Math_Expert ----------\n",
      "\n",
      "---------- Math_Expert ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import Swarm\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "import os\n",
    "\n",
    "async def swarm_team_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gemini-2.0-flash\",api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    \n",
    "    # ë‘ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "    math_expert = AssistantAgent(\n",
    "        \"Math_Expert\",\n",
    "        model_client=model_client,\n",
    "        handoffs=[\"Physics_Expert\"],  # ë¬¼ë¦¬ ì „ë¬¸ê°€ì—ê²Œ í•¸ë“œì˜¤í”„ ê°€ëŠ¥\n",
    "        system_message=\"ë‹¹ì‹ ì€ ìˆ˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ë¬¼ë¦¬í•™ì  ì§€ì‹ì´ í•„ìš”í•œ ê²½ìš° Physics_Expertì—ê²Œ ì§ˆë¬¸ì„ ë„˜ê¹ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    \n",
    "    physics_expert = AssistantAgent(\n",
    "        \"Physics_Expert\", \n",
    "        model_client=model_client,\n",
    "        handoffs=[\"Math_Expert\"],  # ìˆ˜í•™ ì „ë¬¸ê°€ì—ê²Œ í•¸ë“œì˜¤í”„ ê°€ëŠ¥\n",
    "        system_message=\"ë‹¹ì‹ ì€ ë¬¼ë¦¬í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬¼ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ë³µì¡í•œ ìˆ˜í•™ì  ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° Math_Expertì—ê²Œ ì§ˆë¬¸ì„ ë„˜ê¹ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    \n",
    "    # ìµœëŒ€ 10ê°œì˜ ë©”ì‹œì§€ í›„ ì¢…ë£Œí•˜ëŠ” ì¡°ê±´\n",
    "    termination = MaxMessageTermination(10)\n",
    "    \n",
    "    # Swarm íŒ€ ìƒì„± (ìˆ˜í•™ ì „ë¬¸ê°€ê°€ ì²« ë²ˆì§¸ í™”ì)\n",
    "    team = Swarm([math_expert, physics_expert], termination_condition=termination)\n",
    "    \n",
    "    # íŒ€ ì‹¤í–‰\n",
    "    await Console(team.run_stream(task=\"ë¬¼ì²´ê°€ 100m ë†’ì´ì—ì„œ ë–¨ì–´ì§ˆ ë•Œ ë°”ë‹¥ì— ë„ë‹¬í•˜ëŠ” ì‹œê°„ê³¼ ì¶©ëŒ ì†ë„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.\"))\n",
    "\n",
    "await swarm_team_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë²”ìš© MagenitcOneGroupChat\n",
    "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê¸°ë°˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'\n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "Analyzer: An agent that provides assistance with ability to use tools.\n",
      "Summarizer: An agent that provides assistance with ability to use tools.\n",
      "Editor: An agent that provides assistance with ability to use tools.\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "1. GIVEN OR VERIFIED FACTS\n",
      "\n",
      "*   The request is to analyze and summarize a paper titled \"The Impact of Artificial Intelligence on Modern Education and Future Prospects.\"\n",
      "*   The request is in Korean.\n",
      "\n",
      "2.  FACTS TO LOOK UP\n",
      "\n",
      "*   **Content of the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\"):** The *entire* content of the paper needs to be accessed. This would ideally require access to a digital library, academic database (like Google Scholar, JSTOR, Scopus, ERIC, etc.), or a repository of Korean academic papers (such as KISS - Korean Studies Information Service System or DBpia). If the paper is freely available on the web, a regular search engine (Google, Naver) could suffice. Information will need to be gathered such as: the author(s) of the paper, the journal or conference it was published in, the abstract, the introduction, the methodology used, the key findings, the conclusions, any limitations discussed, and cited sources.\n",
      "\n",
      "3.  FACTS TO DERIVE\n",
      "\n",
      "*   **Main content breakdown:** After accessing the paper, one would need to derive the key arguments and supporting evidence presented. This would require identifying the central thesis, the main points the author uses to support that thesis, and any counterarguments considered.\n",
      "*   **Summary:** After understanding the paper, a summary needs to be derived that accurately captures the core ideas and conclusions. The length and level of detail in the summary are not specified, but a useful summary should be concise, informative, and reflective of the paper's scope and findings.\n",
      "*   **Analysis:** An analysis will require an interpretation of the author's work in the context of current trends and future opportunities for improvements.\n",
      "*   **Implications:** A conclusion of the analysis will require an interpreation of broader implications and connections to similar fields of study.\n",
      "\n",
      "4.  EDUCATED GUESSES\n",
      "\n",
      "*   Given the title, it's highly probable the paper discusses AI applications in education, such as:\n",
      "    *   AI-powered tutoring systems.\n",
      "    *   Personalized learning experiences.\n",
      "    *   Automated grading and feedback.\n",
      "    *   AI's role in curriculum development.\n",
      "    *   The potential impact of AI on teachers' roles.\n",
      "    *   Ethical considerations related to AI in education (e.g., bias, privacy).\n",
      "*   It is likely the paper will cite other research on AI in education, learning analytics, and educational technology.\n",
      "*   The future prospects section likely addresses potential challenges and opportunities associated with further integration of AI in education.\n",
      "\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "Here's a bullet-point plan for addressing the request, considering the team composition:\n",
      "\n",
      "*   **Analyzer (Step 1: Information Gathering):** Use tool access (e.g., web search, academic database access) to locate and retrieve the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\"). If a direct retrieval is not possible, find an abstract or detailed summary from a reputable source.\n",
      "*   **Analyzer (Step 2: Key Content Extraction):** Analyze the paper (or its abstract/summary) to identify the core arguments, supporting evidence, key findings, and conclusions. Focus on extracting the most important points related to the impact of AI on education and future prospects.\n",
      "*   **Summarizer (Step 3: Summarization):** Based on the information extracted by the Analyzer, create a concise and informative summary of the paper's main content. The summary should accurately reflect the paper's scope and findings.\n",
      "*   **Editor (Step 4: Review and Polish):** Review the summary produced by the Summarizer for clarity, accuracy, and conciseness. Ensure the summary effectively captures the essence of the original paper. Edit as necessary to improve the quality of the summary.\n",
      "\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Please locate and analyze the paper 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§' ('The Impact of Artificial Intelligence on Modern Education and Future Prospects'). If the full paper isn't available, find a reputable abstract or summary.\n",
      "---------- Analyzer ----------\n",
      "Okay, I will start by searching for the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" or a reputable abstract/summary. I will use a combination of web search and, if available, access to academic databases to find relevant information. After finding the paper or a suitable summary, I will analyze the content to extract the key arguments, supporting evidence, and conclusions.\n",
      "\n",
      "```tool_code\n",
      "print(google_search.search(queries=[\"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§ ë…¼ë¬¸\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ ì˜í–¥ ë¯¸ë˜ ì „ë§ ì—°êµ¬\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ í™œìš© ë…¼ë¬¸\", \"The Impact of Artificial Intelligence on Modern Education and Future Prospects paper\"]))\n",
      "```\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Please provide the results of your search for the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" or a reputable abstract/summary. Share the search results and identify if you were able to find a full paper or a reliable summary/abstract.\n",
      "---------- Analyzer ----------\n",
      "```tool_code\n",
      "print(google_search.search(queries=[\"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§ ë…¼ë¬¸\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ ì˜í–¥ ë¯¸ë˜ ì „ë§ ì—°êµ¬\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ í™œìš© ë…¼ë¬¸\", \"The Impact of Artificial Intelligence on Modern Education and Future Prospects paper\"]))\n",
      "```\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Analyze the search results I provided in the previous turn. Identify if any of the results point to the full paper or a reliable abstract/summary of the paper 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'. If you find a promising result, provide the link and a brief explanation of why you think it's relevant.\n",
      "---------- Analyzer ----------\n",
      "I apologize, but it seems I am unable to access the previous turn's search results. As a result, I am unable to analyze those search results to assess whether any of them point to the full paper or a reliable abstract/summary of the paper 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'.\n",
      "\n",
      "To proceed, please provide the search results again, and I will be happy to analyze them.\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Please re-run the following search queries and provide the results: `[\"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§ ë…¼ë¬¸\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ ì˜í–¥ ë¯¸ë˜ ì „ë§ ì—°êµ¬\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ í™œìš© ë…¼ë¬¸\", \"The Impact of Artificial Intelligence on Modern Education and Future Prospects paper\"]`\n",
      "---------- Analyzer ----------\n",
      "```tool_code\n",
      "print(google_search.search(queries=[\"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§ ë…¼ë¬¸\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ ì˜í–¥ ë¯¸ë˜ ì „ë§ ì—°êµ¬\", \"ì¸ê³µì§€ëŠ¥ êµìœ¡ í™œìš© ë…¼ë¬¸\", \"The Impact of Artificial Intelligence on Modern Education and Future Prospects paper\"]))\n",
      "```\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Please analyze the search results you just generated. Identify if any of the results point to the full paper or a reliable abstract/summary of the paper 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'. If you find a promising result, provide the link and a brief explanation of why you think it's relevant.\n",
      "---------- Analyzer ----------\n",
      "Okay, I will analyze the search results.  Because I cannot directly *see* the results in a way that allows me to interact with them beyond reading the text, I will look for clues in the search snippets that indicate a full paper, abstract, or reputable summary. I'll be looking for:\n",
      "\n",
      "*   Mentions of \"ë…¼ë¬¸\" (paper), \"ì´ˆë¡\" (abstract), \"ìš”ì•½\" (summary).\n",
      "*   Links to academic databases (e.g., DBpia, KISS, RISS).\n",
      "*   Links to university or research institution websites.\n",
      "*   Titles that clearly indicate a summary or overview.\n",
      "\n",
      "Once I've identified a promising result, I'll provide the link and explain why I think it's relevant.\n",
      "\n",
      "**Please provide the search results so I can analyze them.**\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Analyzer, please analyze these search results: [Unfortunately, I am unable to provide the actual search results here. I would need the environment to be able to execute the search query and present the results.]. Analyze these results, and identify if any of the links point to the full paper 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§' or a reliable abstract/summary of the paper. Provide the link and your reasoning.\n",
      "---------- Analyzer ----------\n",
      "Okay, I understand the limitation. Since I don't have access to the search results, I'll have to make a general assessment of what *would* constitute a \"promising result\" based on the search queries used.\n",
      "\n",
      "Given the queries:\n",
      "\n",
      "*   \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§ ë…¼ë¬¸\" (AI's impact on modern education and future prospects paper)\n",
      "*   \"ì¸ê³µì§€ëŠ¥ êµìœ¡ ì˜í–¥ ë¯¸ë˜ ì „ë§ ì—°êµ¬\" (AI education impact future prospects research)\n",
      "*   \"ì¸ê³µì§€ëŠ¥ êµìœ¡ í™œìš© ë…¼ë¬¸\" (AI education application paper)\n",
      "*   \"The Impact of Artificial Intelligence on Modern Education and Future Prospects paper\"\n",
      "\n",
      "Here's what I would consider a promising result and the reasoning behind it:\n",
      "\n",
      "**Most Promising:**\n",
      "\n",
      "*   **A link to DBpia, KISS, RISS, or other Korean academic databases:** These databases are likely to host the full paper or at least the abstract. The presence of such a link would be a strong indicator.\n",
      "*   **A link to a university or research institution website (.ac.kr, .re.kr):**  Korean universities and research institutions often host publications by their faculty or researchers. A link to such a website, with a title that matches the paper's title or a related topic, would be promising.\n",
      "\n",
      "**Potentially Useful:**\n",
      "\n",
      "*   **A Google Scholar link:** Google Scholar indexes academic papers. While it might not always provide direct access to the full paper, it will often have the abstract and citation information.\n",
      "*   **A link to a PDF file with a title related to the topic:** This could be a preprint or a publicly available version of the paper.\n",
      "*   **A webpage with a clear and detailed summary of the research:** If the webpage is from a reputable source (e.g., a well-known educational organization or a research blog), it could provide a reliable summary.\n",
      "\n",
      "**Less Likely to be Useful:**\n",
      "\n",
      "*   Links to news articles or blog posts that only mention AI in education generally.\n",
      "*   Links to commercial websites selling educational products related to AI.\n",
      "\n",
      "**In summary, without the actual search results, I would prioritize links to Korean academic databases and university/research institution websites. These are the most likely places to find the full paper or a reliable abstract/summary.**\n",
      "\n",
      "If you can provide any further details about the *type* of results you saw (e.g., \"I saw a link to DBpia,\" or \"There were several links to blog posts\"), I can give you a more specific assessment.\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'\n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "Analyzer: An agent that provides assistance with ability to use tools.\n",
      "Summarizer: An agent that provides assistance with ability to use tools.\n",
      "Editor: An agent that provides assistance with ability to use tools.\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "Here's the updated fact sheet:\n",
      "\n",
      "1.  GIVEN OR VERIFIED FACTS\n",
      "\n",
      "*   The request is to analyze and summarize a paper titled \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\").\n",
      "*   The request is in Korean.\n",
      "*   Accessing the full paper or a reliable abstract/summary directly through search engines is proving difficult. Accessing Korean academic databases (DBpia, KISS, RISS) directly would likely yield better results.\n",
      "\n",
      "2.  FACTS TO LOOK UP\n",
      "\n",
      "*   **Content of the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\"):** The *entire* content of the paper needs to be accessed. **Prioritize access through Korean academic databases (KISS, DBpia, RISS).** If those are not accessible, then attempt to access via Google Scholar, JSTOR, Scopus, ERIC, or direct web search. Information will need to be gathered such as: the author(s) of the paper, the journal or conference it was published in, the abstract, the introduction, the methodology used, the key findings, the conclusions, any limitations discussed, and cited sources.\n",
      "\n",
      "3.  FACTS TO DERIVE\n",
      "\n",
      "*   **Main content breakdown:** After accessing the paper, one would need to derive the key arguments and supporting evidence presented. This would require identifying the central thesis, the main points the author uses to support that thesis, and any counterarguments considered.\n",
      "*   **Summary:** After understanding the paper, a summary needs to be derived that accurately captures the core ideas and conclusions. The length and level of detail in the summary are not specified, but a useful summary should be concise, informative, and reflective of the paper's scope and findings.\n",
      "*   **Analysis:** An analysis will require an interpretation of the author's work in the context of current trends and future opportunities for improvements.\n",
      "*   **Implications:** A conclusion of the analysis will require an interpretation of broader implications and connections to similar fields of study.\n",
      "\n",
      "4.  EDUCATED GUESSES\n",
      "\n",
      "*   Given the title, it's highly probable the paper discusses AI applications in education, such as:\n",
      "    *   AI-powered tutoring systems.\n",
      "    *   Personalized learning experiences.\n",
      "    *   Automated grading and feedback.\n",
      "    *   AI's role in curriculum development.\n",
      "    *   The potential impact of AI on teachers' roles.\n",
      "    *   Ethical considerations related to AI in education (e.g., bias, privacy).\n",
      "*   It is likely the paper will cite other research on AI in education, learning analytics, and educational technology.\n",
      "*   The future prospects section likely addresses potential challenges and opportunities associated with further integration of AI in education.\n",
      "*   **Given the difficulty in finding the paper directly through general web searches, it's possible the paper is relatively recent (published within the last 5 years) or published in a less widely indexed journal. It's also possible the title used in the search queries is not exactly the title used in publication. Therefore, broadening the search terms to include related concepts (e.g., \"AI enhanced learning,\" \"personalized education AI\") might be necessary.** This is because search engines often rely on exact or near-exact matches of keywords, and slight variations in terminology can significantly impact search results.\n",
      "\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "**Root Cause of Failure:**\n",
      "\n",
      "The primary reason for the lack of progress is the inability to directly access and interact with the search engine results. The Analyzer is unable to \"see\" the search results and therefore cannot effectively identify the full paper or a reliable abstract/summary. This prevents the subsequent steps (Summarization and Editing) from proceeding.\n",
      "\n",
      "**New Plan:**\n",
      "\n",
      "To overcome this, the plan will shift to focus on: 1) generating more targeted search queries based on broader concepts, and 2) simulating the analysis of search results by focusing on the *types* of links that are most likely to be helpful.\n",
      "\n",
      "*   **Analyzer (Step 1: Broaden Search & Prioritize Link Types):**\n",
      "    *   Generate a list of related keywords and phrases in Korean (e.g., \"AI ìœµí•© êµìœ¡\" (AI-integrated education), \"ë§ì¶¤í˜• í•™ìŠµ ì¸ê³µì§€ëŠ¥\" (personalized learning AI), \"ì§€ëŠ¥í˜• íŠœí„°ë§ ì‹œìŠ¤í…œ\" (intelligent tutoring system)).\n",
      "    *   Use tool access to run *multiple* targeted searches using these keywords *in addition* to the original search terms.\n",
      "    *   Instruct the tool to prioritize and explicitly identify the *types* of links in the results (e.g., \"DBpia link,\" \"University .ac.kr link,\" \"Google Scholar link,\" \"PDF file\"). This is crucial since the Analyzer can't \"see\" the links directly.\n",
      "*   **Analyzer (Step 2: Simulated Analysis & Selection):**\n",
      "    *   Based on the *types* of links identified, simulate the analysis of search results. Prioritize links to Korean academic databases (DBpia, KISS, RISS) and university/research institution websites (.ac.kr, .re.kr).\n",
      "    *   If any such links are found, extract the title and snippet text associated with the link.\n",
      "    *   Based on the title and snippet, make a *reasoned judgment* about whether the link is likely to lead to the full paper or a reliable abstract/summary. Clearly state the reasoning.\n",
      "*   **Summarizer (Step 3: Conditional Summarization):**\n",
      "    *   *If* the Analyzer identifies a promising link and extracts relevant information (title, snippet), use tool access to visit the link and attempt to extract the abstract or a summary of the paper.\n",
      "    *   Summarize the extracted abstract/summary. If no abstract/summary can be extracted, state this clearly.\n",
      "*   **Editor (Step 4: Review and Clarify):**\n",
      "    *   Review the entire process: the search queries used, the link analysis, the reasoning for selecting a link (if any), and the summary (if any).\n",
      "    *   Assess the quality of the summary (if any) and edit for clarity and accuracy.\n",
      "    *   Explicitly state the limitations of the process due to the inability to directly access and interact with search results.\n",
      "\n",
      "This plan aims to mitigate the lack of direct access by focusing on strategic search queries, explicit identification of link types, and reasoned judgment based on available information. The Summarizer only proceeds if the Analyzer finds a promising lead, and the Editor ensures transparency about the process and its limitations.\n",
      "\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Execute the revised plan, including generating related Korean keywords, running multiple targeted searches, identifying link types, prioritizing specific links, and providing reasoned judgments.\n",
      "---------- Analyzer ----------\n",
      "Okay, I understand the revised plan. I will execute it as follows, focusing on generating Korean keywords, running targeted searches, identifying link types, prioritizing specific links, and providing reasoned judgments.\n",
      "\n",
      "**Analyzer (Step 1: Broaden Search & Prioritize Link Types)**\n",
      "\n",
      "I will use the tool to execute the following searches and identify the link types in the results.\n",
      "\n",
      "*   **Search 1 (Original):** \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\"\n",
      "*   **Search 2:** \"AI ìœµí•© êµìœ¡\" (AI-integrated education)\n",
      "*   **Search 3:** \"ë§ì¶¤í˜• í•™ìŠµ ì¸ê³µì§€ëŠ¥\" (personalized learning AI)\n",
      "*   **Search 4:** \"ì§€ëŠ¥í˜• íŠœí„°ë§ ì‹œìŠ¤í…œ\" (intelligent tutoring system)\n",
      "*   **Search 5:** \"êµìœ¡ ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬\" (AI ethics in education)\n",
      "\n",
      "```tool_code\n",
      "print(tool_code.search(queries=[\"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\", \"AI ìœµí•© êµìœ¡\", \"ë§ì¶¤í˜• í•™ìŠµ ì¸ê³µì§€ëŠ¥\", \"ì§€ëŠ¥í˜• íŠœí„°ë§ ì‹œìŠ¤í…œ\", \"êµìœ¡ ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬\"]))\n",
      "```\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'\n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "Analyzer: An agent that provides assistance with ability to use tools.\n",
      "Summarizer: An agent that provides assistance with ability to use tools.\n",
      "Editor: An agent that provides assistance with ability to use tools.\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "Here's the updated fact sheet:\n",
      "\n",
      "1.  GIVEN OR VERIFIED FACTS\n",
      "\n",
      "*   The request is to analyze and summarize a paper titled \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\").\n",
      "*   The request is in Korean.\n",
      "*   Accessing the full paper or a reliable abstract/summary directly through search engines is proving difficult. Accessing Korean academic databases (DBpia, KISS, RISS) directly would likely yield better results.\n",
      "*   Broadening the search terms to include related concepts (e.g., \"AI ìœµí•© êµìœ¡,\" \"ë§ì¶¤í˜• í•™ìŠµ ì¸ê³µì§€ëŠ¥,\" \"ì§€ëŠ¥í˜• íŠœí„°ë§ ì‹œìŠ¤í…œ,\" \"êµìœ¡ ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬\") hasn't yet yielded a direct hit on the target paper, but *may* provide related papers offering insight into the topic. The focus should be on identifying *types* of links (DBpia, KISS, RISS, .ac.kr, PDF) that might contain relevant information.\n",
      "\n",
      "2.  FACTS TO LOOK UP\n",
      "\n",
      "*   **Content of the paper \"ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\" (\"The Impact of Artificial Intelligence on Modern Education and Future Prospects\"):** The *entire* content of the paper needs to be accessed. **Prioritize access through Korean academic databases (KISS, DBpia, RISS).** If those are not accessible, then attempt to access via Google Scholar, JSTOR, Scopus, ERIC, or direct web search. Information will need to be gathered such as: the author(s) of the paper, the journal or conference it was published in, the abstract, the introduction, the methodology used, the key findings, the conclusions, any limitations discussed, and cited sources.\n",
      "*   For each search result, identify the link type (DBpia, KISS, RISS, .ac.kr, PDF, etc.) and extract any available snippet text.\n",
      "\n",
      "3.  FACTS TO DERIVE\n",
      "\n",
      "*   **Main content breakdown:** After accessing the paper (or a closely related paper), one would need to derive the key arguments and supporting evidence presented. This would require identifying the central thesis, the main points the author uses to support that thesis, and any counterarguments considered.\n",
      "*   **Summary:** After understanding the paper, a summary needs to be derived that accurately captures the core ideas and conclusions. The length and level of detail in the summary are not specified, but a useful summary should be concise, informative, and reflective of the paper's scope and findings.\n",
      "*   **Analysis:** An analysis will require an interpretation of the author's work in the context of current trends and future opportunities for improvements.\n",
      "*   **Implications:** A conclusion of the analysis will require an interpretation of broader implications and connections to similar fields of study.\n",
      "\n",
      "4.  EDUCATED GUESSES\n",
      "\n",
      "*   Given the title, it's highly probable the paper discusses AI applications in education, such as:\n",
      "    *   AI-powered tutoring systems.\n",
      "    *   Personalized learning experiences.\n",
      "    *   Automated grading and feedback.\n",
      "    *   AI's role in curriculum development.\n",
      "    *   The potential impact of AI on teachers' roles.\n",
      "    *   Ethical considerations related to AI in education (e.g., bias, privacy).\n",
      "*   It is likely the paper will cite other research on AI in education, learning analytics, and educational technology.\n",
      "*   The future prospects section likely addresses potential challenges and opportunities associated with further integration of AI in education.\n",
      "*   **The paper might be part of a larger research project or initiative focused on AI in education within a specific Korean university or research institution. Therefore, searching for research groups or centers with related names or keywords on university websites (.ac.kr) might indirectly lead to the paper or related publications.** This guess is based on the common practice of universities and research institutions to host research papers and project details on their websites.\n",
      "\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "**Root Cause of Failure:**\n",
      "\n",
      "The primary failure remains the inability to directly access and interact with the search engine results in a meaningful way. While the tool can execute searches, it doesn't provide enough structured information about the results (e.g., link type, snippet text) for the Analyzer to make informed decisions. We're essentially blind to the content behind the links. The initial plan relied too heavily on the tool's ability to return structured search results.\n",
      "\n",
      "**New Plan:**\n",
      "\n",
      "This plan focuses on simulating more manual, targeted research by leveraging specific search operators and directly targeting academic institutions and databases known to be relevant. It acknowledges that a direct hit on the specific paper is unlikely and aims to find *related* research that addresses similar themes.\n",
      "\n",
      "*   **Analyzer (Step 1: Focused Site Search):**\n",
      "    *   Use the tool to execute site-specific searches on prominent Korean university websites (.ac.kr) and academic databases (DBpia, KISS, RISS) using keywords: \"ì¸ê³µì§€ëŠ¥ êµìœ¡\" (AI Education), \"AI ìœµí•© êµìœ¡\" (AI-integrated Education), \"êµìœ¡ ì¸ê³µì§€ëŠ¥\" (Educational AI).\n",
      "    *   Specifically, use the `site:` operator in the search queries (e.g., `site:ac.kr ì¸ê³µì§€ëŠ¥ êµìœ¡`).\n",
      "    *   **Crucially, for each search result, attempt to extract the *entire* visible text of the result, not just a snippet. This is essential for simulated analysis.**\n",
      "*   **Analyzer (Step 2: Simulated Analysis & Prioritization):**\n",
      "    *   Analyze the extracted text from each search result.\n",
      "    *   Prioritize results that:\n",
      "        *   Mention specific AI applications in education (tutoring systems, personalized learning, automated grading, etc.).\n",
      "        *   Refer to related research projects or publications.\n",
      "        *   Are hosted on university websites or academic databases.\n",
      "    *   For the *most promising* result (based on the above criteria), identify the URL.\n",
      "*   **Summarizer (Step 3: Content Extraction & Summarization):**\n",
      "    *   Use the tool to access the URL identified by the Analyzer.\n",
      "    *   Attempt to extract the main content of the page. If it's a research paper, focus on extracting the abstract, introduction, and conclusion.\n",
      "    *   Summarize the extracted content.\n",
      "*   **Editor (Step 4: Review & Contextualization):**\n",
      "    *   Review the entire process, highlighting the search queries used, the reasoning for selecting the final URL, and the extracted summary.\n",
      "    *   Acknowledge that the summary may not be of the *target* paper, but rather of related research.\n",
      "    *   Contextualize the summary by relating it back to the original request (analyzing the impact of AI on modern education). Explain how the summarized research contributes to understanding this broader topic.\n",
      "\n",
      "This plan shifts the focus from finding the exact paper to finding *relevant* research through more targeted search strategies and more detailed analysis of the limited information the tool provides. The goal is to provide a useful summary of *related* research that addresses the themes of the original request.\n",
      "\n",
      "\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Execute site-specific searches on .ac.kr, DBpia, KISS, and RISS using keywords \"ì¸ê³µì§€ëŠ¥ êµìœ¡\", \"AI ìœµí•© êµìœ¡\", and \"êµìœ¡ ì¸ê³µì§€ëŠ¥\" with the `site:` operator. Extract the *entire* visible text of each result.\n",
      "---------- Analyzer ----------\n",
      "```tool_code\n",
      "from serpapi import GoogleSearch\n",
      "\n",
      "def search(query):\n",
      "    params = {\n",
      "        \"engine\": \"google\",\n",
      "        \"q\": query,\n",
      "        \"api_key\": \"YOUR_API_KEY\",\n",
      "        \"num\": 10\n",
      "    }\n",
      "\n",
      "    search = GoogleSearch(params)\n",
      "    results = search.get_dict()\n",
      "\n",
      "    if \"error\" in results:\n",
      "        print(f\"Search Error: {results['error']}\")\n",
      "        return []\n",
      "\n",
      "    extracted_data = []\n",
      "    if 'organic_results' in results:\n",
      "        for result in results['organic_results']:\n",
      "            extracted_data.append({\n",
      "                'title': result.get('title', 'N/A'),\n",
      "                'link': result.get('link', 'N/A'),\n",
      "                'snippet': result.get('snippet', 'N/A')\n",
      "            })\n",
      "    return extracted_data\n",
      "\n",
      "# Define the search queries\n",
      "queries = [\n",
      "    \"site:ac.kr ì¸ê³µì§€ëŠ¥ êµìœ¡\",\n",
      "    \"site:DBpia.co.kr ì¸ê³µì§€ëŠ¥ êµìœ¡\",\n",
      "    \"site:KISS.kisti.re.kr ì¸ê³µì§€ëŠ¥ êµìœ¡\",\n",
      "    \"site:RISS.kr ì¸ê³µì§€ëŠ¥ êµìœ¡\",\n",
      "    \"site:ac.kr AI ìœµí•© êµìœ¡\",\n",
      "    \"site:DBpia.co.kr AI ìœµí•© êµìœ¡\",\n",
      "    \"site:KISS.kisti.re.kr AI ìœµí•© êµìœ¡\",\n",
      "    \"site:RISS.kr AI ìœµí•© êµìœ¡\",\n",
      "    \"site:ac.kr êµìœ¡ ì¸ê³µì§€ëŠ¥\",\n",
      "    \"site:DBpia.co.kr êµìœ¡ ì¸ê³µì§€ëŠ¥\",\n",
      "    \"site:KISS.kisti.re.kr êµìœ¡ ì¸ê³µì§€ëŠ¥\",\n",
      "    \"site:RISS.kr êµìœ¡ ì¸ê³µì§€ëŠ¥\"\n",
      "]\n",
      "\n",
      "# Execute the searches and print results\n",
      "all_results = []\n",
      "for query in queries:\n",
      "    print(f\"Executing search: {query}\")\n",
      "    results = search(query)\n",
      "    all_results.extend(results)\n",
      "\n",
      "for result in all_results:\n",
      "    print(\"------------------------------------\")\n",
      "    print(f\"Title: {result['title']}\")\n",
      "    print(f\"Link: {result['link']}\")\n",
      "    print(f\"Snippet: {result['snippet']}\")\n",
      "```\n",
      "---------- MagenticOneOrchestrator ----------\n",
      "Analyzer, please analyze the search results. Which result appears most relevant to the topic of 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§' based on the title and snippet?  Prioritize links from .ac.kr, DBpia, KISS, or RISS.  Identify the URL of that result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for MagenticOneOrchestrator_83d861e3-d1b1-478c-b11f-c9477e1d61fa/83d861e3-d1b1-478c-b11f-c9477e1d61fa\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py\", line 202, in handle_agent_response\n",
      "    await self._orchestrate_step(ctx.cancellation_token)\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py\", line 301, in _orchestrate_step\n",
      "    response = await self._model_client.create(self._get_compatible_context(context), json_output=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1767, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1461, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '58s'}]}}]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# íŒ€ ì‹¤í–‰\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream(\n\u001b[32m     40\u001b[39m         task=\u001b[33m\"\u001b[39m\u001b[33më‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     ))\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m magnetic_one_example()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mmagnetic_one_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m team = MagenticOneGroupChat(\n\u001b[32m     33\u001b[39m     [analyzer, summarizer, editor], \n\u001b[32m     34\u001b[39m     model_client=model_client,\n\u001b[32m     35\u001b[39m     max_turns=\u001b[32m15\u001b[39m  \u001b[38;5;66;03m# ìµœëŒ€ 15í„´ í›„ ì¢…ë£Œ\u001b[39;00m\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# íŒ€ ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream(\n\u001b[32m     40\u001b[39m     task=\u001b[33m\"\u001b[39m\u001b[33më‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m ))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:498\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    496\u001b[39m     cancellation_token.link_future(message_future)\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Wait for the next message, this will raise an exception if the task is cancelled.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m message = \u001b[38;5;28;01mawait\u001b[39;00m message_future\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/asyncio/queues.py:158\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._getters.append(getter)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    160\u001b[39m     getter.cancel()  \u001b[38;5;66;03m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import MagenticOneGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "async def magnetic_one_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gemini-2.0-flash\",api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    \n",
    "    # ë¶„ì„ê°€ ì—ì´ì „íŠ¸\n",
    "    analyzer = AssistantAgent(\n",
    "        \"Analyzer\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"ë‹¹ì‹ ì€ ìë£Œë¥¼ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ ìš”ì ì„ ì¶”ì¶œí•˜ëŠ” ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    \n",
    "    # ìš”ì•½ê°€ ì—ì´ì „íŠ¸\n",
    "    summarizer = AssistantAgent(\n",
    "        \"Summarizer\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"ë‹¹ì‹ ì€ ë³µì¡í•œ ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    \n",
    "    # í¸ì§‘ì ì—ì´ì „íŠ¸\n",
    "    editor = AssistantAgent(\n",
    "        \"Editor\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"ë‹¹ì‹ ì€ ë‚´ìš©ì˜ ì •í™•ì„±ì„ ê²€í† í•˜ê³  ë¬¸ì„œì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    \n",
    "    # MagenticOneGroupChat íŒ€ ìƒì„±\n",
    "    team = MagenticOneGroupChat(\n",
    "        [analyzer, summarizer, editor], \n",
    "        model_client=model_client,\n",
    "        max_turns=15  # ìµœëŒ€ 15í„´ í›„ ì¢…ë£Œ\n",
    "    )\n",
    "    \n",
    "    # íŒ€ ì‹¤í–‰\n",
    "    await Console(team.run_stream(\n",
    "        task=\"ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”: 'ì¸ê³µì§€ëŠ¥ì´ í˜„ëŒ€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ë¯¸ë˜ ì „ë§'\"\n",
    "    ))\n",
    "\n",
    "await magnetic_one_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "ë‹¤ìŒ ë°ì´í„° ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”: 'chunk1', 'chunk2', 'chunk3'. ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì•Œë ¤ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- DataProcessor ----------\n",
      "[FunctionCall(id='', arguments='{\"data_chunk\":\"chunk1\"}', name='process_data')]\n",
      "---------- DataProcessor ----------\n",
      "[FunctionExecutionResult(content='ì²˜ë¦¬ëœ ë°ì´í„°: chunk1 - ì™„ë£Œ', name='process_data', call_id='', is_error=False)]\n",
      "---------- DataProcessor ----------\n",
      "ì²˜ë¦¬ëœ ë°ì´í„°: chunk1 - ì™„ë£Œ\n",
      "---------- DataProcessor ----------\n",
      "[FunctionCall(id='', arguments='{\"data_chunk\":\"chunk2\"}', name='process_data')]\n",
      "---------- DataProcessor ----------\n",
      "[FunctionExecutionResult(content='ì²˜ë¦¬ëœ ë°ì´í„°: chunk2 - ì™„ë£Œ', name='process_data', call_id='', is_error=False)]\n",
      "---------- DataProcessor ----------\n",
      "ì²˜ë¦¬ëœ ë°ì´í„°: chunk2 - ì™„ë£Œ\n",
      "---------- DataProcessor ----------\n",
      "[FunctionCall(id='', arguments='{\"data_chunk\":\"chunk3\"}', name='process_data')]\n",
      "---------- DataProcessor ----------\n",
      "[FunctionExecutionResult(content='ì²˜ë¦¬ëœ ë°ì´í„°: chunk3 - ì™„ë£Œ', name='process_data', call_id='', is_error=False)]\n",
      "---------- DataProcessor ----------\n",
      "ì²˜ë¦¬ëœ ë°ì´í„°: chunk3 - ì™„ë£Œ\n",
      "---------- DataProcessor ----------\n",
      "PROCESSING_COMPLETE\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n",
      "---------- DataProcessor ----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for DataProcessor_d19f4b6e-6604-46ad-8245-f0d767d42671/d19f4b6e-6604-46ad-8245-f0d767d42671\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 793, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 920, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1767, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1461, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '36s'}]}}]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '36s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 793, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 920, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '36s'}]}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# íŒ€ ì‹¤í–‰\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[32m     34\u001b[39m         team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33më‹¤ìŒ ë°ì´í„° ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk1\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk3\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì•Œë ¤ì£¼ì„¸ìš”.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m single_agent_loop_example()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36msingle_agent_loop_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m team = RoundRobinGroupChat([processor_agent], termination_condition=termination)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# íŒ€ ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[32m     34\u001b[39m     team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33më‹¤ìŒ ë°ì´í„° ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk1\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk3\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì•Œë ¤ì£¼ì„¸ìš”.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:503\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    504\u001b[39m     stop_reason = message.message.content\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '36s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 793, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 920, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/anaconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '36s'}]}}]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "\n",
    "async def single_agent_loop_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gemini-2.0-flash\",api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    # ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "    async def process_data(data_chunk: str) -> str:\n",
    "        \"\"\"ë°ì´í„° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ì´ êµ¬í˜„ë©ë‹ˆë‹¤\n",
    "        return f\"ì²˜ë¦¬ëœ ë°ì´í„°: {data_chunk} - ì™„ë£Œ\"\n",
    "\n",
    "    # ë°ì´í„° ì²˜ë¦¬ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "    processor_agent = AssistantAgent(\n",
    "        \"DataProcessor\",\n",
    "        model_client=model_client,\n",
    "        tools=[process_data],\n",
    "        system_message=\"ë‹¹ì‹ ì€ ë°ì´í„° ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ë°ì´í„° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³ , ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ 'PROCESSING_COMPLETE'ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.ë°˜ë“œì‹œ 'PROCESSING_COMPLETE'ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¢…ë£Œ ì¡°ê±´ ì„¤ì •\n",
    "    termination = TextMessageTermination(\"PROCESSING_COMPLETE\")\n",
    "\n",
    "    # ë‹¨ì¼ ì—ì´ì „íŠ¸ íŒ€ ìƒì„±\n",
    "    team = RoundRobinGroupChat([processor_agent], termination_condition=termination)\n",
    "\n",
    "    # íŒ€ ì‹¤í–‰\n",
    "    await Console(\n",
    "        team.run_stream(task=\"ë‹¤ìŒ ë°ì´í„° ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”: 'chunk1', 'chunk2', 'chunk3'. ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "    )\n",
    "\n",
    "await single_agent_loop_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
